{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf3dc3e",
   "metadata": {},
   "source": [
    "# Project: BERT for Banking FAQ Classification\n",
    "This project uses BERT (Bidirectional Encoder Representations from Transformers) to classify questions from a banking FAQ dataset into predefined categories such as 'accounts', 'loans', etc.\n",
    "\n",
    "**Why this matters:**\n",
    "- Enhances customer support automation in banking\n",
    "- Enables quick and accurate routing of queries\n",
    "- Utilizes state-of-the-art transformer-based NLP techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f841805",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "We begin by importing the `pandas` library to work with structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc4a61ea-9924-409a-9b4a-4de962b4e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178ed16",
   "metadata": {},
   "source": [
    "### Load and Display Dataset\n",
    "The dataset containing banking FAQs is loaded using `pandas.read_csv()`. We use `.head()` to preview the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6a052a-4bd3-4257-bb1a-7434a121a9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the documents required for opening a ...</td>\n",
       "      <td>Following documents are required to open a Cur...</td>\n",
       "      <td>accounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I transfer my Current Account from one bra...</td>\n",
       "      <td>Yes, Current Accounts can be transferred from ...</td>\n",
       "      <td>accounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My present status is NRI. What extra documents...</td>\n",
       "      <td>NRI/PIO can open the proprietorship/partnershi...</td>\n",
       "      <td>accounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the documents required for opening a ...</td>\n",
       "      <td>Following documents are required for opening a...</td>\n",
       "      <td>accounts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What documents are required to change the addr...</td>\n",
       "      <td>Following documents are required to change the...</td>\n",
       "      <td>accounts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What are the documents required for opening a ...   \n",
       "1  Can I transfer my Current Account from one bra...   \n",
       "2  My present status is NRI. What extra documents...   \n",
       "3  What are the documents required for opening a ...   \n",
       "4  What documents are required to change the addr...   \n",
       "\n",
       "                                              Answer     Class  \n",
       "0  Following documents are required to open a Cur...  accounts  \n",
       "1  Yes, Current Accounts can be transferred from ...  accounts  \n",
       "2  NRI/PIO can open the proprietorship/partnershi...  accounts  \n",
       "3  Following documents are required for opening a...  accounts  \n",
       "4  Following documents are required to change the...  accounts  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank = pd.read_csv(\"BankFAQs.csv\")\n",
    "bank.head()  # Load CSV data into a DataFrame  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba7db5",
   "metadata": {},
   "source": [
    "### Import Label Encoder\n",
    "The `LabelEncoder` from scikit-learn is imported to convert categorical labels (e.g., 'accounts') into numerical form for model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c3d2e2-e49a-4881-b9aa-87b9d1836177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9fe958-da4a-4f65-a425-9d0fb125a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f765af-8dc1-4317-99fc-65c6071177cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank [\"label\"] = label_encoder.fit_transform (bank[\"Class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c972c-fd30-4262-a81b-6f312458959a",
   "metadata": {},
   "source": [
    "### Train-Test Split of Dataset\n",
    "\n",
    "We split the dataset into training and validation sets using an 80/20 ratio.  \n",
    "- `stratify=bank[\"label\"]` ensures that the distribution of classes remains consistent across train and validation sets (important for classification problems).  \n",
    "- `random_state=42` ensures reproducibility of the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f466508-dff3-47df-ae48-06e60909d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e13e640-490c-4784-a43e-e361dc4e1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, val_text, train_label, val_label = train_test_split(\n",
    "    bank[\"Question\"].tolist(),  # Input text data\n",
    "    bank[\"label\"].tolist(),     # Corresponding target labels\n",
    "    test_size=0.2,              # 20% of data used for validation\n",
    "    stratify=bank[\"label\"],     # Maintain class distribution in split\n",
    "    random_state=42             # Reproducible results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b8cfe-c952-453b-9f43-f6c886529b04",
   "metadata": {},
   "source": [
    "### Tokenization Using BERT Tokenizer\n",
    "\n",
    "We use the `BertTokenizer` from Hugging Face Transformers to tokenize the text data.  \n",
    "- `bert-base-uncased` is a pre-trained lowercase BERT model.\n",
    "- Tokenization converts raw text into token IDs that the BERT model can understand.\n",
    "- We apply truncation and padding to ensure all sequences are of the same length (`max_length=128`), which is essential for batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f086a6-41ab-4ba1-9457-37fdaa18002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc1ec1b-d680-4ec3-bfcf-ada51a6ecd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_text,truncation = True ,padding = True ,max_length=128)\n",
    "val_encodings = tokenizer(val_text,truncation=True,padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa85bfb-0d54-4958-945c-51773f029cb0",
   "metadata": {},
   "source": [
    "### Creating a Custom PyTorch Dataset for BERT\n",
    "\n",
    "We define a custom `Dataset` class to wrap the tokenized inputs and labels so they can be used with PyTorch's `DataLoader`.\n",
    "\n",
    "- `__init__`: Stores the input encodings and labels.\n",
    "- `__len__`: Returns the number of samples (required by PyTorch).\n",
    "- `__getitem__`: Retrieves one data sample as a dictionary of tensors (`input_ids`, `attention_mask`, and `labels`) â€” this is the format BERT expects.\n",
    "\n",
    "Finally, we create `train_dataset` and `val_dataset` objects for training and validation respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f7bac11-aeab-4bdd-ad87-3a997cdb7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define a custom PyTorch Dataset for BERT\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings  # Tokenized inputs (input_ids, attention_mask)\n",
    "        self.labels = labels        # Corresponding class labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)     # Total number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Create a dictionary with input tensors for a single sample\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])  # Add label tensor\n",
    "        return item\n",
    "\n",
    "# Create dataset objects for training and validation\n",
    "train_dataset = Dataset(train_encodings, train_label)\n",
    "val_dataset = Dataset(val_encodings, val_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b6e62-5930-438f-aaff-1090c37f91c4",
   "metadata": {},
   "source": [
    "### Load Pretrained BERT Model for Sequence Classification\n",
    "\n",
    "We load a pre-trained BERT model (`bert-base-uncased`) and fine-tune it for our multi-class classification task.\n",
    "\n",
    "- `BertForSequenceClassification` is a wrapper over BERT that adds a classification head (a linear layer) on top.\n",
    "- `num_labels` is set to the number of unique classes in our target variable.\n",
    "- This prepares the model to predict one of the N classes given a banking-related question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e472a1de-b3dd-4552-90cd-5d47b5c44068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Determine the number of target classes from the label encoder\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# Load pre-trained BERT model with a classification head on top\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',  # Base BERT model (lowercased)\n",
    "    num_labels=num_labels # Output layer adjusted to number of classes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc07c64-f2fb-4ce4-a87d-6e69f0756bc4",
   "metadata": {},
   "source": [
    "### Define Training Arguments for BERT Fine-Tuning\n",
    "\n",
    "We use `TrainingArguments` from Hugging Face to configure how the model will be trained:\n",
    "\n",
    "- `output_dir`: Where the model checkpoints and predictions will be saved.\n",
    "- `evaluation_strategy=\"epoch\"`: Evaluate the model at the end of each epoch.\n",
    "- `per_device_train_batch_size` & `per_device_eval_batch_size`: Batch sizes during training and evaluation.\n",
    "- `num_train_epochs`: Total number of passes through the training data.\n",
    "- `weight_decay`: Regularization to reduce overfitting.\n",
    "- `logging_dir`: Directory to save training logs.\n",
    "- `logging_steps`: How often to log training progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "708e1fb1-575c-4aa6-9be7-c0dc3fe31361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kunal\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kunal\\anaconda3\\envs\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                 # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"epoch\",           # Evaluate after each epoch\n",
    "    per_device_train_batch_size=16,        # Batch size for training\n",
    "    per_device_eval_batch_size=16,         # Batch size for evaluation\n",
    "    num_train_epochs=4,                    # Number of training epochs\n",
    "    weight_decay=0.01,                     # Regularization (L2 penalty)\n",
    "    logging_dir='./logs',                  # Directory to save logs\n",
    "    logging_steps=10                       # Log training metrics every 10 steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a39d5-6df3-49fb-832a-c2c35dc218e8",
   "metadata": {},
   "source": [
    "### Initialize the Hugging Face Trainer\n",
    "\n",
    "We use the `Trainer` class to manage the full training and evaluation pipeline:\n",
    "\n",
    "- `model`: The BERT model for sequence classification.\n",
    "- `args`: Training arguments we defined earlier (batch size, epochs, logging, etc.).\n",
    "- `train_dataset`: Training data in the format expected by the model.\n",
    "- `eval_dataset`: Validation data used to evaluate the model after each epoch.\n",
    "\n",
    "The `Trainer` handles optimization, backpropagation, evaluation, and logging internally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec86928-fb74-4563-8548-30b6807602ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                   # Pretrained BERT model with classification head\n",
    "    args=training_args,            # Training configuration (epochs, batch size, etc.)\n",
    "    train_dataset=train_dataset,   # Tokenized and labeled training data\n",
    "    eval_dataset=val_dataset       # Tokenized and labeled validation data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9963cd4-093c-4e9a-9b70-17923f7489b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='356' max='356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [356/356 46:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.541000</td>\n",
       "      <td>0.461093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.293099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.271037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.274052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=356, training_loss=0.35204597361636963, metrics={'train_runtime': 2778.6927, 'train_samples_per_second': 2.041, 'train_steps_per_second': 0.128, 'total_flos': 119511228541200.0, 'train_loss': 0.35204597361636963, 'epoch': 4.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42792ef9-8d8b-4048-8b0c-6e5d6cf44215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27405211329460144, 'eval_runtime': 41.4655, 'eval_samples_per_second': 8.561, 'eval_steps_per_second': 0.555, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3f530a9-23ee-4f5e-b986-141ec705f239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.27405211329460144,\n",
       " 'eval_model_preparation_time': 0.0064,\n",
       " 'eval_accuracy': 0.9295774647887324,\n",
       " 'eval_runtime': 43.7587,\n",
       " 'eval_samples_per_second': 8.113,\n",
       " 'eval_steps_per_second': 0.526}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate again\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0888420d-8476-41c9-80df-14a0872c97fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-faq-model\\\\tokenizer_config.json',\n",
       " 'bert-faq-model\\\\special_tokens_map.json',\n",
       " 'bert-faq-model\\\\vocab.txt',\n",
       " 'bert-faq-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"bert-faq-model\")\n",
    "tokenizer.save_pretrained(\"bert-faq-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ec62c-d7cc-4748-ba64-c0c921d0e757",
   "metadata": {},
   "source": [
    "### Inference: Predict Label for New Question\n",
    "\n",
    "We define a helper function `predict_faq(question)` that:\n",
    "- Tokenizes the input question using the same tokenizer used during training.\n",
    "- Feeds the tokenized input into the trained BERT model.\n",
    "- Extracts the predicted class by selecting the index with the highest logit score.\n",
    "- Converts the predicted class index back to the original label using the `LabelEncoder`.\n",
    "\n",
    "This allows us to make real-time predictions for new user questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50b492b6-d80d-4f95-9067-f8ce880941ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "security\n"
     ]
    }
   ],
   "source": [
    "def predict_faq(question):\n",
    "    # Tokenize the input question and convert to PyTorch tensors\n",
    "    inputs = tokenizer(\n",
    "        question, \n",
    "        return_tensors=\"pt\",        # Return as PyTorch tensors\n",
    "        truncation=True,            # Truncate if longer than max length\n",
    "        padding=True,               # Pad shorter sequences\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    # Disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)   # Get model predictions\n",
    "\n",
    "    logits = outputs.logits         # Raw model outputs (logits)\n",
    "    predicted_class_id = torch.argmax(logits, dim=1).item()  # Index of highest logit\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]  # Decode label\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "question = \"How can I apply for a credit card?\"\n",
    "print(predict_faq(question))  # Output: predicted label for the question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774086b-08c0-4125-9f33-1677bcafb465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
